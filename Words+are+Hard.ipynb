{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words are Hard!\n",
    "### Exploration with DeepNLP for Ontological Word-Sense Disambiguation\n",
    "### By Uthman Apatira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sentence:\n",
    "\n",
    "> _Alexis won the bet agaist her car._\n",
    "\n",
    "Quite confusing even for a native English speaker, much less for a deterministic computer. First of all, who exactly is _her_ referring to here? In English, Alexis is a unisex name, so _her_ might very well be referring to Alexis herself or perhaps some other party. The usage of pronouns makes the sentence appear more natural, but also introduces **referential ambiguity**. The way we (people) normally overcome this is by discourse analysis, that is, by examining the immediately preceeding sentences.\n",
    "\n",
    "Additionally, is the word _bet_ being used as a noun or as a verb? Humans readily identify it as being a noun in this case; but a computer making use of NLP techniques would only have a single feature (TF-IDF) or a single word embedding (Glove/Word2Vec) to describe _bet_ in any context used, noun, verb, or otherwise. This is called **lexical ambiguity**.\n",
    "\n",
    "Finally, the sample sentence above also exhibits **syntactical ambiguity** because it is not clear if Alexis was betting against her own car, i.e. she assumed her car would not be able to fulfill the conditions of the bet, _or_ if Alexis was physically stationed next to her car.\n",
    "\n",
    "Part of the beauty of language is its intrisic ambiguity. It's honestly a meracle we can communite and understand each other at all! This notebook will demonstrate some best practices and dive into a few modern NLP techniques that can help computer models perform better at natural language understanding. Stated formally, this notebook will cover some explaratory data analysis ad modeling with ontological word-sense disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Named Entity Tagging](#Named-Entity-Tagging)\n",
    "- [Classical Sentiment Analysis](#Classical-Sentiment-Analysis)\n",
    "- [Bayesian Hyperparameter Tuning](#Bayesian-Hyperparameter-Tuning)\n",
    "- [Embedding Training](#Embedding-Training)\n",
    "- [Sentiment Analysis with CNNs](#Sentiment-Analysis-with-CNNs)\n",
    "- [Sentiment Analysis with RNNs](#Sentiment-Analysis-with-RNNs)\n",
    "- [Sentiment Analysis with RNNs+CNNs](#Sentiment-Analysis-with-RNNs+CNNs)\n",
    "- [POS-Augmentation for Embeddings](#POS-Augmentation-for-Embeddings)\n",
    "- [Transfer Learning](#Transfer-Learning)\n",
    "- [Reflections](#Reflections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our journey starts with _Named Entity Tagging_, also known as named entity recognition. A named entity is just some entity that deserves to have a name =). The process of tagging them seeks to classify named entities into some pre-defined categories such as the names of persons, organizations, places, etc.\n",
    "\n",
    "By properly tagging named entities, our desire is to have a strong ally in our battle of ontological word-sense disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, nltk\n",
    "from nltk.chunk import tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/authman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/authman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/authman/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/authman/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downlod some nltk packages, if necessary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_lines = [\n",
    "    'Alexis won the bet agaist her car.',\n",
    "    'Free us from the tyranny of technology, making a connected life a more human experience.',\n",
    "    'He started working at Microsoft in Seattle, Washington.',\n",
    "    'San Francisco has a IBM office.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NETagger(line):\n",
    "    results = []\n",
    "    \n",
    "    # A potnetial improvment :\n",
    "    # Some organizations, e.g. 'JC Penny', places 'San Francisco'\n",
    "    # and name-nouns, 'Brian Kursar' are multi-word. We can \n",
    "    # combine them..\n",
    "\n",
    "    # Tag the individual parts of speech;\n",
    "    line = nltk.pos_tag(line)\n",
    "\n",
    "    # Now, extract named entities via parse tree\n",
    "    line = nltk.ne_chunk(line)\n",
    "\n",
    "    # Finally, covert the tree to a list of\n",
    "    # tag-tuples: (word, pos, tag)\n",
    "    line = nltk.tree2conlltags(line)\n",
    "\n",
    "    for word,pos,tag in line:\n",
    "        if 'GPE' in tag:            tag = 'PLACE'\n",
    "        elif 'ORGANIZATION' in tag: tag = 'ORGANIZATION'\n",
    "        elif 'NN' in pos:           tag = 'NOUN'\n",
    "        elif 'VB' in pos:           tag = 'VERB'\n",
    "        else:                       tag = 'OTHER'\n",
    "        results.append(tag)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAlexis won the bet agaist her car.\u001b[0m\n",
      "Alexis is NOUN\n",
      "won is VERB\n",
      "bet is NOUN\n",
      "agaist is NOUN\n",
      "car is NOUN\n",
      "\n",
      "\u001b[1;31mFree us from the tyranny of technology, making a connected life a more human experience.\u001b[0m\n",
      "tyranny is NOUN\n",
      "technology is NOUN\n",
      "making is VERB\n",
      "life is NOUN\n",
      "experience is NOUN\n",
      "\n",
      "\u001b[1;31mHe started working at Microsoft in Seattle, Washington.\u001b[0m\n",
      "started is VERB\n",
      "working is VERB\n",
      "Microsoft is ORGANIZATION\n",
      "Seattle is PLACE\n",
      "Washington is PLACE\n",
      "\n",
      "\u001b[1;31mSan Francisco has a IBM office.\u001b[0m\n",
      "San is PLACE\n",
      "Francisco is NOUN\n",
      "has is VERB\n",
      "IBM is ORGANIZATION\n",
      "office is NOUN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for line in sys.stdin:\n",
    "for line in sample_lines:\n",
    "    print('\\x1b[1;31m' + line + '\\x1b[0m')\n",
    "    \n",
    "    # Chop it up, we can use regex to do this directly\n",
    "    line = nltk.word_tokenize(line)\n",
    "    \n",
    "    results = NETagger(line)\n",
    "    for i,word in enumerate(line):\n",
    "        if results[i] == 'OTHER': continue\n",
    "        print(word, 'is', results[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! NLTK's `pos_tag` has support for many more parts of speech, and the `ne_chunk` chunker can identify many more named entities that what we've listed. By for this demo notebook, our dataset will be sufficiently small that if we get too crazy at this point, there potentially won't be enough overlap. That stated, to see the full tagset, execute the cell below. For our purposes, we're limiting the output to just nouns, verbs, places, and organizations. Everything else will just be marked as `'other'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what it gives us...\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here by 'classical', I mean imploring the use of term-frequency, inverse-term-frequency. We will stack this transformer under either a logistic regression classifier for our final classification.\n",
    "\n",
    "To get us started, I'll be using the `Large Movie Review Dataset`, which is a very balanced dataset of 25k positive and 25k negative reviews of different movies, with no more than 30 reviews per particular movie. So that I don't forget:\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite messy, all the reviews are in separate files. Let's bunch them up into .feather files for easy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, gc, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# For the fancy jupyter status bar\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg = os.listdir(\"net/aclImdb/train/neg/\")\n",
    "train_pos = os.listdir(\"net/aclImdb/train/pos/\")\n",
    "test_neg = os.listdir(\"net/aclImdb/test/neg/\")\n",
    "test_pos = os.listdir(\"net/aclImdb/test/pos/\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_pos), len(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:02<00:00, 5912.86it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5912.26it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5985.13it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5866.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "\n",
    "for file in tqdm(train_neg):\n",
    "    with open(\"net/aclImdb/train/neg/\" + file) as fhandler:\n",
    "        train.append([0, fhandler.read()])\n",
    "        \n",
    "for file in tqdm(train_pos):\n",
    "    with open(\"net/aclImdb/train/pos/\" + file) as fhandler:\n",
    "        train.append([1, fhandler.read()])\n",
    "        \n",
    "train = pd.DataFrame(train, columns=['sentiment','comment'])\n",
    "\n",
    "\n",
    "test = []\n",
    "for file in tqdm(test_neg):\n",
    "    with open(\"net/aclImdb/test/neg/\" + file) as fhandler:\n",
    "        test.append([0, fhandler.read()])\n",
    "        \n",
    "for file in tqdm(test_pos):\n",
    "    with open(\"net/aclImdb/test/pos/\" + file) as fhandler:\n",
    "        test.append([1, fhandler.read()])\n",
    "        \n",
    "test  = pd.DataFrame(test, columns=['sentiment','comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stash as feather for easy future access\n",
    "train.to_feather('net/train.ftr')\n",
    "test.to_feather('net/test.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is how to load it in the future\n",
    "train = pd.read_feather('net/train.ftr')\n",
    "test  = pd.read_feather('net/test.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I read all the reviews here AFTER watching thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie was not very entertaining, certainl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie is entertaining enough due to an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This self proclaimed \"very talented artist\" ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>had some lovely poetic bits but is really just...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            comment\n",
       "0          0  I read all the reviews here AFTER watching thi...\n",
       "1          0  This movie was not very entertaining, certainl...\n",
       "2          0  This movie is entertaining enough due to an ex...\n",
       "3          0  This self proclaimed \"very talented artist\" ha...\n",
       "4          0  had some lovely poetic bits but is really just..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>I first saw this film by chance when I was vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>Much better than expected. Good family flick -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>I am trying to find somewhere to purchase a DV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite the title and unlike some other storie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>Delightful! It never pretends to be a masterpi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            comment\n",
       "24995          1  I first saw this film by chance when I was vis...\n",
       "24996          1  Much better than expected. Good family flick -...\n",
       "24997          1  I am trying to find somewhere to purchase a DV...\n",
       "24998          1  Despite the title and unlike some other storie...\n",
       "24999          1  Delightful! It never pretends to be a masterpi..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.4, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by training the tdidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tdidf = TfidfVectorizer(\n",
    "    lowercase   = True,\n",
    "    stop_words  = 'english',\n",
    "    ngram_range = (1, 2),\n",
    "    max_df      = 0.4,\n",
    "    min_df      = 5,\n",
    "    binary      = False, \n",
    "    smooth_idf  = True\n",
    ")\n",
    "\n",
    "# We'll train our vctorizer using both train+test\n",
    "tdidf.fit(train.comment.append(test.comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this dataset didn't already provide us a train/test split, we could have\n",
    "# use KFold, and re-ran our model a few times over.\n",
    "train_idf = tdidf.transform(train.comment)\n",
    "test_idf  = tdidf.transform(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88708\n"
     ]
    }
   ],
   "source": [
    "# Now, train a LReg classifier over our IDF vector.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "                               \n",
    "lreg = LogisticRegression(\n",
    "    dual    = False,  # For large dsets, this can speed things up . . .\n",
    "    C       = 5.125,  # Our main hyperprameter\n",
    "    n_jobs  = 1\n",
    ")\n",
    "\n",
    "lreg.fit(train_idf, train.sentiment)\n",
    "predict = lreg.predict(test_idf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test.sentiment, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 88% Accuracy, our score wasn't that great. There are many ways we can improve it. We can add tri-grams to the idf vectorizer. We can also introduce character n-grams as well. No matter what we do, it is always a good idea to tune our hyperparameters using our hold-out set.\n",
    "\n",
    "One technique for doing this is grid-search over our hyperparameter space. This seems to be going out of fashion because Bayesian optimization is all the craze these days, so let's go with that. The deep learner, a 'frequentists', believes a model's parameters are fixed and the data is random. A bayesian learner on the other hand, holds that the _data_ is fixed, but the parameters are random. If you think about it, this makes sense because when a model is trained, the data is fixed and isn't random anymore. I won't go into more detail than that here, but I'll say that there are packages that will take the conditional probabilities out of you :-). Let's use the hyperopt to explore a possible hyperparam space and find the best `C`, `max_df`, and `min_df` values to maximize our accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, define the search space:\n",
    "space = {\n",
    "    'ngram_range': hp.choice('x_ngram_range', [(1,1), (1,2), (1,3)]),\n",
    "    'min_df'     : hp.quniform('x_min_df', 3, 6, 1),\n",
    "    'max_df'     : hp.uniform('x_max_df', 0.4, 1.0),\n",
    "    'C'          : hp.uniform('x_C', 0.1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then define our objective function\n",
    "def objective(space):\n",
    "    # This is where KFold would come into play. We actually have a small\n",
    "    # DSet and using ALL of it (test+train) would be beneficial. We can\n",
    "    # Concatenate both sets, then do KFold x-validation here. But for this\n",
    "    # demo, we'll just use the test set as-is.\n",
    "    global train, test\n",
    "    \n",
    "    # Debugging:\n",
    "    print(space)\n",
    "    \n",
    "    tdidf = TfidfVectorizer(\n",
    "        lowercase   = True,\n",
    "        stop_words  = 'english',\n",
    "        ngram_range = space['ngram_range'],\n",
    "        max_df      = space['max_df'],\n",
    "        min_df      = int(space['min_df']),\n",
    "        binary      = False, \n",
    "        smooth_idf  = True\n",
    "    )\n",
    "    tdidf.fit(train.comment.append(test.comment))\n",
    "    \n",
    "    lreg = LogisticRegression(\n",
    "        dual    = False,\n",
    "        C       = space['C']\n",
    "    )\n",
    "    lreg.fit(train_idf, train.sentiment)\n",
    "    \n",
    "    predict = lreg.predict(test_idf)\n",
    "    score   = accuracy_score(test.sentiment, predict)\n",
    "    print('\\t', score, '\\n')\n",
    "    \n",
    "    # 1-score, since we're going to attempt to minimize this value\n",
    "    return 1.0 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.0606887543996963, 'max_df': 0.5618433074066356, 'min_df': 4.0, 'ngram_range': (1, 2)}\n",
      "\t 0.88624 \n",
      "\n",
      "{'C': 9.346095270971665, 'max_df': 0.4811437711583409, 'min_df': 4.0, 'ngram_range': (1, 3)}\n",
      "\t 0.88496 \n",
      "\n",
      "{'C': 8.86599382635101, 'max_df': 0.9835505665598256, 'min_df': 5.0, 'ngram_range': (1, 3)}\n",
      "\t 0.88512 \n",
      "\n",
      "{'C': 2.531381705909005, 'max_df': 0.5198443883233169, 'min_df': 6.0, 'ngram_range': (1, 3)}\n",
      "\t 0.88576 \n",
      "\n",
      "{'C': 6.930366842543968, 'max_df': 0.5525014488080969, 'min_df': 4.0, 'ngram_range': (1, 2)}\n",
      "\t 0.88604 \n",
      "\n",
      "{'C': 4.189597138737334, 'max_df': 0.4453876676869535, 'min_df': 6.0, 'ngram_range': (1, 2)}\n",
      "\t 0.88708 \n",
      "\n",
      "{'C': 3.407494420130505, 'max_df': 0.6123552378516658, 'min_df': 3.0, 'ngram_range': (1, 2)}\n",
      "\t 0.8866 \n",
      "\n",
      "{'C': 7.030869445011589, 'max_df': 0.5526430492433034, 'min_df': 4.0, 'ngram_range': (1, 2)}\n",
      "\t 0.886 \n",
      "\n",
      "{'C': 9.100614699103993, 'max_df': 0.4092647835386215, 'min_df': 4.0, 'ngram_range': (1, 1)}\n",
      "\t 0.88512 \n",
      "\n",
      "{'C': 8.180403568445342, 'max_df': 0.8340350734358322, 'min_df': 6.0, 'ngram_range': (1, 2)}\n",
      "\t 0.88552 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The Trials object will store details of each iteration\n",
    "trials = Trials()\n",
    "\n",
    "# Run it:\n",
    "best = fmin(\n",
    "    objective,\n",
    "    space     = space,\n",
    "    algo      = tpe.suggest,\n",
    "    max_evals = 10\n",
    ")\n",
    "\n",
    "# Get the values of the optimal parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it looks like the tuned-TfIDF gets us all the way up to **88.704% accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten as far as we believe we can with TFIDF, let's move on to neural networks.\n",
    "\n",
    "We could feed in our sparse TF-IDF representation as input into our net (think one hot encoding). But instead, we're going to try to create an embedding, or lower level representation of our language data. This will involve us first tokenizing our textual corpus into indices, and then assigning a random, trainable weight vector to each word.\n",
    "\n",
    "Before starting, we're going to need to set a fixed input size for our comments. How do we know what size to use? Let's see what the data tells us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npM79L/PtuvySqqkcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f32ec749320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = train.comment.str.count('\\S+').astype(np.uint16)\n",
    "\n",
    "plt.hist(words, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**250** will be a very competitive number of words. It'll capture most of the next of most of our comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding, Conv1D \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size  = 100000   # We won't use more than 100K words\n",
    "max_text_len    = 250      # We'll only examine up to the first 100 words of each review\n",
    "\n",
    "embedding_dimension = 100    # Each word will be represented with a dense, 100 unit vector\n",
    "embedding_maxvocab  = 500000 # Our language model will know up to 500k words, probably more than in our toy dset\n",
    "embedding_cbow_win  = 5      # Window size for unsuperivsd text context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 29146.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments = train.comment.append(test.comment).values\n",
    "all_comments = [text.text_to_word_sequence(comment) for comment in tqdm(all_comments)]\n",
    "\n",
    "w2v = Word2Vec(size=embedding_dimension, window=embedding_cbow_win, max_vocab_size=embedding_maxvocab)\n",
    "w2v.build_vocab(all_comments)\n",
    "w2v.train(all_comments, total_examples=w2v.corpus_count, epochs=5)\n",
    "\n",
    "w2v.save('net/embedding.w2v')\n",
    "\n",
    "del all_comments; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we allow our embedding vocabulary to be quite big, but this is because while building the corpus, we don't know which words are important yet. Later on once we start supervised learning, we can prune accordingly.\n",
    "\n",
    "Next up, let's build a mapper to tokenize our text (change the words -> vocab indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(\n",
    "    list(train.comment.fillna('NA').values) + list(test.comment.fillna('NA').values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenizer):\n",
    "    # Just like we did with TfIdf, let's remove stopwords\n",
    "    # From our vocabulary\n",
    "    \n",
    "    C_MIN_WF = 4\n",
    "    C_MIN_DF = 4\n",
    "    C_MAX_DF = tokenizer.document_count * 0.4\n",
    "    print('MAX_DF', C_MAX_DF)\n",
    "\n",
    "    whack_words = [w for w,c in tokenizer.word_counts.items() if c < C_MIN_WF] + [w for w,c in tokenizer.word_docs.items() if c < C_MIN_DF or c > C_MAX_DF]\n",
    "    whack_words = list(set(whack_words))\n",
    "    for word in whack_words:\n",
    "        del tokenizer.word_counts[word], tokenizer.word_index[word], tokenizer.word_docs[word]\n",
    "        #tokenizer.num_words -= 1\n",
    "\n",
    "    # Since we deleted some entris, relabel (fix) the word indices\n",
    "    word_index_keys = tokenizer.word_index.keys()\n",
    "    for i,word in enumerate(word_index_keys):\n",
    "        tokenizer.word_index[word] = i+1\n",
    "    print('Deleting', len(whack_words), 'words that appear too frequently or too infrequently')\n",
    "\n",
    "    word_index_keys = tokenizer.word_index.keys()\n",
    "    print(len(word_index_keys), 'words left')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_DF 20000.0\n",
      "Deleting 79412 words that appear too frequently or too infrequently\n",
      "44840 words left\n"
     ]
    }
   ],
   "source": [
    "tokenizer = remove_stopwords(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We just built the model, so no need to load it.\n",
    "# But this is how that would be done:\n",
    "w2v = Word2Vec.load('net/embedding.w2v')\n",
    "\n",
    "\n",
    "# Start by initializing our embedding matrix [VocabSize x EmbeddingDimension]\n",
    "word_index = tokenizer.word_index\n",
    "nb_words   = min(max_vocab_size, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dimension), dtype=np.float64)\n",
    "\n",
    "# Since we're using a self trained embedding, our expectation is that there\n",
    "# will be no out-of-vocabulary words present... but I'll leave that code in\n",
    "# there anyway because I'll be coming back to it:\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_vocab_size: continue\n",
    "    try:\n",
    "        embedding_vector = w2v.wv[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44840, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can execute the tokenizer:\n",
    "train_seq = tokenizer.texts_to_sequences(train.comment)\n",
    "test_seq  = tokenizer.texts_to_sequences(test.comment)\n",
    "\n",
    "# Add 0-padding to reach max_text_len size\n",
    "train_seq = sequence.pad_sequences(train_seq, maxlen=max_text_len)\n",
    "test_seq  = sequence.pad_sequences(test_seq,  maxlen=max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I', array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          298,   797,    92,    59,   106,   371,  1260,  1140,   509,\n",
       "           27,   183,   189,  5196,   124,  1806,   288,   113,    60,\n",
       "        11607,  2002,  4062,   251, 24346,  1151,  2208,   823,    58,\n",
       "           38,   964,   172,   183,   111,   227,  5836,   161,   157,\n",
       "           82,    66,  3763,    36,  2354,   229,   299,   262,  5758,\n",
       "           25,   431,    34,   180,  8048,    85,  1362,    97,  5329,\n",
       "         7019,  2929,  2009,    90,   290,   371,   798,  6015,  1467,\n",
       "         1579,  2577,    17,    89,    35,   459, 15074,    12,    44,\n",
       "         2988,  3750,  4869,   410,  5837,   187, 12269,   161,    20,\n",
       "          140,    76, 12269,    21,    83,   342, 16699, 21900,  1151,\n",
       "           20,   140,    76, 21167,   119, 16346, 11608,   575,    20,\n",
       "           32,   342, 24347,    44,    36,   161,  8417, 17473,   995,\n",
       "        11117,   290, 18370,   379,   183,   483,   837,   713,   214,\n",
       "         5587,  2950,   156,   967,    43,   363,  2208,    13,    54,\n",
       "            5,    25,    34,  5836,   161,   125,   100,   183,   672,\n",
       "          112,  2515,    20,    54,    57,    16,  3384,    16,    68,\n",
       "           49, 15075,  7617,   990,  7267, 12103,  5678,   425,   159,\n",
       "         2292,  1230,     6,  7160,    57,   119,    34,   111,   229,\n",
       "           81,    21,   138,   584,     7,   759,    27,   125,   433,\n",
       "          397,  5837,   187,  1980,  2482,   159,  1970,  1998,   111,\n",
       "         7020,    86,   473,   266,    43,     5,    13,  2419,   528,\n",
       "           26,    20,   336, 24348,  6815,  4516,   346,    42,  2109,\n",
       "           24,  4474,    17,   374,  2334,   808,  2854], dtype=int32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word \"I\" is mapped to this 100-unit vector:\n",
    "train.comment[0][0], train_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of prep work, but now we're ready to build a NNet model to perform sentiment analysis. For the loss function, I'll use binary log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 250, 100)          4484000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 32)           9632      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,493,665\n",
      "Trainable params: 9,665\n",
      "Non-trainable params: 4,484,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_cnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_cnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple model. Notice that our embedding layer is now frozen. Our understanding is that the embedding layer encoders some information about 'English', which includes syntax, semantics, and pragmatics to some degree. We don't want to change that. Rather, we'll leave it to our convolutional filters to learn the target mapping of good / bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "24544/25000 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8047\n",
      "Epoch 00001: val_loss improved from inf to 0.37713, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 5s 196us/step - loss: 0.4475 - acc: 0.8038 - val_loss: 0.3771 - val_acc: 0.8485\n",
      "Epoch 2/10\n",
      "24608/25000 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8225\n",
      "Epoch 00002: val_loss improved from 0.37713 to 0.34884, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.4099 - acc: 0.8227 - val_loss: 0.3488 - val_acc: 0.8600\n",
      "Epoch 3/10\n",
      "24592/25000 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8321\n",
      "Epoch 00003: val_loss improved from 0.34884 to 0.33718, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 149us/step - loss: 0.3930 - acc: 0.8324 - val_loss: 0.3372 - val_acc: 0.8644\n",
      "Epoch 4/10\n",
      "24592/25000 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8366\n",
      "Epoch 00004: val_loss improved from 0.33718 to 0.33500, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.3866 - acc: 0.8366 - val_loss: 0.3350 - val_acc: 0.8693\n",
      "Epoch 5/10\n",
      "24592/25000 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8410\n",
      "Epoch 00005: val_loss improved from 0.33500 to 0.32319, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.3796 - acc: 0.8405 - val_loss: 0.3232 - val_acc: 0.8714\n",
      "Epoch 6/10\n",
      "24912/25000 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8419\n",
      "Epoch 00006: val_loss did not improve\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.3785 - acc: 0.8419 - val_loss: 0.3239 - val_acc: 0.8696\n",
      "Epoch 7/10\n",
      "24544/25000 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8440\n",
      "Epoch 00007: val_loss improved from 0.32319 to 0.32256, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.3724 - acc: 0.8445 - val_loss: 0.3226 - val_acc: 0.8744\n",
      "Epoch 8/10\n",
      "24608/25000 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8434\n",
      "Epoch 00008: val_loss improved from 0.32256 to 0.32066, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 150us/step - loss: 0.3717 - acc: 0.8437 - val_loss: 0.3207 - val_acc: 0.8710\n",
      "Epoch 9/10\n",
      "24608/25000 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8465\n",
      "Epoch 00009: val_loss improved from 0.32066 to 0.31432, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 148us/step - loss: 0.3702 - acc: 0.8470 - val_loss: 0.3143 - val_acc: 0.8734\n",
      "Epoch 10/10\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8443\n",
      "Epoch 00010: val_loss improved from 0.31432 to 0.31299, saving model to net/best_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 150us/step - loss: 0.3661 - acc: 0.8442 - val_loss: 0.3130 - val_acc: 0.8769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32fee205c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_cnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 10,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At **87.44%**, our convlutional model performed _sliiiiightly_ worse than our classical IDF-LReg model (which had **88.704%** accuracy), how disappointing. Some things to note -- we used `kernel_size=3`, and don't have any spatial pooling (only temporal pooling). You can imagine here we're looking at n=3 grams ONLY. Our TFIDF model used n=1,2, and 3 gram combinations. We could adjust our nnet with additional kernels to consider additional n-grams as well, but that won't be explored in this notebook. There are many other ways to improve our network as well, the least of which being tuning the model's hyperparams, but let's take a look at an RNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 250, 100)          4484000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 250, 32)           11328     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,495,361\n",
      "Trainable params: 11,361\n",
      "Non-trainable params: 4,484,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_rnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(16, return_sequences=True))(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_rnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8191\n",
      "Epoch 00001: val_loss improved from inf to 0.34518, saving model to net/best_rnn_model.hdf5\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.4163 - acc: 0.8190 - val_loss: 0.3452 - val_acc: 0.8545\n",
      "Epoch 2/5\n",
      "24944/25000 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8373\n",
      "Epoch 00002: val_loss improved from 0.34518 to 0.33746, saving model to net/best_rnn_model.hdf5\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3811 - acc: 0.8372 - val_loss: 0.3375 - val_acc: 0.8602\n",
      "Epoch 3/5\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8413\n",
      "Epoch 00003: val_loss improved from 0.33746 to 0.33238, saving model to net/best_rnn_model.hdf5\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.3724 - acc: 0.8413 - val_loss: 0.3324 - val_acc: 0.8606\n",
      "Epoch 4/5\n",
      "24976/25000 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8363\n",
      "Epoch 00004: val_loss improved from 0.33238 to 0.33180, saving model to net/best_rnn_model.hdf5\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3779 - acc: 0.8364 - val_loss: 0.3318 - val_acc: 0.8618\n",
      "Epoch 5/5\n",
      "24976/25000 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8344\n",
      "Epoch 00005: val_loss did not improve\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3835 - acc: 0.8344 - val_loss: 0.3494 - val_acc: 0.8566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32ffec6ef0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_rnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since at train and test time, we have the entire sentence available, it makes sense that we use a bidirectional rnn rather than just limiting ourselves to processing the sentence unidirectionally. I've decided to use 16-processing units per direction, just so we can match the total '32' neuron count in our convolutional network. These figures are pulld out of the air, lol.\n",
    "\n",
    "But take a look at that training time. RNNs are a LOTT slower than CNNs because they consume the data recursively. There is some research out there on [helping RNNs perform better in parallel](NLP QRNN: https://arxiv.org/pdf/1803.08240.pdf), but it's is not widely adopted as of yet. We can use a larger batch size to speed things up, but that affects training quality.\n",
    "\n",
    "Using RNNs, we were able to acheive an accuracy of **86.18%** on the sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with RNNs+CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For complteness, let's see what happens when we combine both RNN and CNN processing. I'm going to bump up the dropout amount a bit to bolster regularization. Perhaps some L2 could be useful as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 250, 100)          4484000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 250, 32)           11328     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,495,361\n",
      "Trainable params: 11,361\n",
      "Non-trainable params: 4,484,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def simple_rcnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(16, return_sequences=True))(x)\n",
    "    x = Conv1D(filters=32, kernel_size=1, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_rnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "24976/25000 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8156\n",
      "Epoch 00001: val_loss improved from inf to 0.33774, saving model to net/best_rcnn_model.hdf5\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.4181 - acc: 0.8156 - val_loss: 0.3377 - val_acc: 0.8582\n",
      "Epoch 2/5\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8384\n",
      "Epoch 00002: val_loss did not improve\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.3793 - acc: 0.8384 - val_loss: 0.3438 - val_acc: 0.8553\n",
      "Epoch 3/5\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8421\n",
      "Epoch 00003: val_loss improved from 0.33774 to 0.33717, saving model to net/best_rcnn_model.hdf5\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3743 - acc: 0.8421 - val_loss: 0.3372 - val_acc: 0.8600\n",
      "Epoch 4/5\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8320\n",
      "Epoch 00004: val_loss did not improve\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3884 - acc: 0.8320 - val_loss: 0.3635 - val_acc: 0.8448\n",
      "Epoch 5/5\n",
      "24976/25000 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8304\n",
      "Epoch 00005: val_loss did not improve\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.3980 - acc: 0.8304 - val_loss: 0.3523 - val_acc: 0.8509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32ebcefe48>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_rcnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Augmentation for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, perhaps we can further improve our NNet scores by adding in either named entities or the part of speech information we worked on earlier? There are a few techniques we could use. One would be to train an embedding on our original text corpus converted into POS or NETs. Then, armed with an embedding to describe each distinct POS, we could simply concatenate that onto our input. Another methods would be to OHE the tags and just feed them in as a separate input vector. Let's try out the first method.\n",
    "\n",
    "But first, a quick \"Gotcha!\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'started',\n",
       " 'working',\n",
       " 'at',\n",
       " 'Microsoft',\n",
       " 'in',\n",
       " 'Seattle',\n",
       " ',',\n",
       " 'Washington',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"He started working at Microsoft in Seattle, Washington.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'started', 'working', 'at', 'microsoft', 'in', 'seattle', 'washington']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text_to_word_sequence(\"He started working at Microsoft in Seattle, Washington.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Keras's text tokenizer converts everything to lowercase? That will mess up our NET. So let's use nltk's tokenizer this round instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netag_max_vocab_size       = 6   # place, org, noun, verb, other (0 is empty)\n",
    "netag_embedding_dimension  = 3   # Meh, should do it. min(50, dimen//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process takes a while, so let's use all our (my) available CPU threads to speed it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def build_ne_tags():\n",
    "    global train, test\n",
    "    \n",
    "    NUM_THREADS = 12\n",
    "\n",
    "    all_ne_tags = train.comment.append(test.comment).values\n",
    "    with Pool(NUM_THREADS) as pool:  all_ne_tags = pool.map(nltk.word_tokenize, all_ne_tags)\n",
    "    with Pool(NUM_THREADS) as pool:  all_ne_tags = pool.map(NETagger, all_ne_tags)\n",
    "        \n",
    "    return all_ne_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're in business :-)\n",
    "all_ne_tags = build_ne_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_tag = Word2Vec(size=netag_embedding_dimension, window=embedding_cbow_win, max_vocab_size=netag_max_vocab_size)\n",
    "w2v_tag.build_vocab(all_ne_tags)\n",
    "w2v_tag.train(all_ne_tags, total_examples=w2v.corpus_count, epochs=5)\n",
    "\n",
    "# Store resulting embedding\n",
    "w2v_tag.save('net/net_embedding.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case we're coming in from another run where we have it saved already:\n",
    "w2v_tag = Word2Vec.load('net/net_embedding.w2v')\n",
    "\n",
    "# Stash the data back into our dset\n",
    "train['netags'] = all_ne_tags[:train.shape[0]]\n",
    "test['netags']  = all_ne_tags[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment</th>\n",
       "      <th>netags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I read all the reviews here AFTER watching thi...</td>\n",
       "      <td>[OTHER, VERB, OTHER, OTHER, NOUN, OTHER, VERB,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie was not very entertaining, certainl...</td>\n",
       "      <td>[OTHER, NOUN, VERB, OTHER, OTHER, OTHER, OTHER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie is entertaining enough due to an ex...</td>\n",
       "      <td>[OTHER, NOUN, VERB, VERB, OTHER, OTHER, OTHER,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This self proclaimed \"very talented artist\" ha...</td>\n",
       "      <td>[OTHER, NOUN, VERB, OTHER, OTHER, OTHER, NOUN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>had some lovely poetic bits but is really just...</td>\n",
       "      <td>[VERB, OTHER, OTHER, OTHER, NOUN, OTHER, VERB,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            comment  \\\n",
       "0          0  I read all the reviews here AFTER watching thi...   \n",
       "1          0  This movie was not very entertaining, certainl...   \n",
       "2          0  This movie is entertaining enough due to an ex...   \n",
       "3          0  This self proclaimed \"very talented artist\" ha...   \n",
       "4          0  had some lovely poetic bits but is really just...   \n",
       "\n",
       "                                              netags  \n",
       "0  [OTHER, VERB, OTHER, OTHER, NOUN, OTHER, VERB,...  \n",
       "1  [OTHER, NOUN, VERB, OTHER, OTHER, OTHER, OTHER...  \n",
       "2  [OTHER, NOUN, VERB, VERB, OTHER, OTHER, OTHER,...  \n",
       "3  [OTHER, NOUN, VERB, OTHER, OTHER, OTHER, NOUN,...  \n",
       "4  [VERB, OTHER, OTHER, OTHER, NOUN, OTHER, VERB,...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, rebuild the CNN model to take dual inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOUN': <gensim.models.keyedvectors.Vocab at 0x7f3303da4438>,\n",
       " 'ORGANIZATION': <gensim.models.keyedvectors.Vocab at 0x7f3303da4400>,\n",
       " 'OTHER': <gensim.models.keyedvectors.Vocab at 0x7f3303dc4f60>,\n",
       " 'PLACE': <gensim.models.keyedvectors.Vocab at 0x7f3303da4390>,\n",
       " 'VERB': <gensim.models.keyedvectors.Vocab at 0x7f3303dc4e10>}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_tag.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = {\n",
    "    'oov':          0,\n",
    "    'NOUN':         1,\n",
    "    'ORGANIZATION': 2,\n",
    "    'OTHER':        3,\n",
    "    'PLACE':        4,\n",
    "    'VERB':         5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by initializing our embedding matrix [VocabSize x EmbeddingDimension]\n",
    "net_embedding_matrix = np.zeros((netag_max_vocab_size, netag_embedding_dimension))\n",
    "\n",
    "# Self trained, no OOV:\n",
    "for key,val in tagger.items():\n",
    "    try:\n",
    "        net_embedding_matrix[val] = w2v_tag.wv[key]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.41369095, -0.35485178,  0.25164878],\n",
       "       [-0.92024428, -0.79106778, -7.99419308],\n",
       "       [ 0.16811559, -0.58408523,  0.74580336],\n",
       "       [-6.41222095,  1.95210791, -2.73351336],\n",
       "       [ 0.47213939, -0.67386639,  1.17783999]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool !\n",
    "net_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq_net = train.netags.map(lambda tags: [tagger[tag] for tag in tags])\n",
    "test_seq_net  = test.netags.map(lambda tags: [tagger[tag] for tag in tags])\n",
    "\n",
    "# Add 0-padding to reach max_text_len size\n",
    "train_seq_net = sequence.pad_sequences(train_seq_net, maxlen=max_text_len)\n",
    "test_seq_net  = sequence.pad_sequences(test_seq_net,  maxlen=max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 250, 100)     4484000     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 250, 100)     0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 250, 3)       18          input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 250, 103)     0           spatial_dropout1d_7[0][0]        \n",
      "                                                                 embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 248, 33)      10230       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 33)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 33)           0           global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            34          dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,494,282\n",
      "Trainable params: 10,264\n",
      "Non-trainable params: 4,484,018\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setup our dual-head CNN:\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def intermediate_cnn_model(embedding_matrix, net_embedding_matrix):\n",
    "    \n",
    "    inp  = Input(shape = (max_text_len, ))\n",
    "    inp2 = Input(shape = (max_text_len, ))\n",
    "    \n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    y = Embedding(\n",
    "        netag_max_vocab_size,\n",
    "        netag_embedding_dimension,\n",
    "        weights      = [net_embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp2)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = concatenate([\n",
    "        # Text + NETags\n",
    "        x, y\n",
    "    ])\n",
    "    \n",
    "    x = Conv1D(filters=33, kernel_size=3, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=[inp,inp2], outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = intermediate_cnn_model(embedding_matrix, net_embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "24768/25000 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8022\n",
      "Epoch 00001: val_loss improved from inf to 0.35944, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.4462 - acc: 0.8020 - val_loss: 0.3594 - val_acc: 0.8535\n",
      "Epoch 2/10\n",
      "24928/25000 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8227\n",
      "Epoch 00002: val_loss improved from 0.35944 to 0.34107, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.4112 - acc: 0.8227 - val_loss: 0.3411 - val_acc: 0.8640\n",
      "Epoch 3/10\n",
      "24944/25000 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8300\n",
      "Epoch 00003: val_loss improved from 0.34107 to 0.33948, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.4004 - acc: 0.8301 - val_loss: 0.3395 - val_acc: 0.8653\n",
      "Epoch 4/10\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8368\n",
      "Epoch 00004: val_loss improved from 0.33948 to 0.32948, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.3872 - acc: 0.8368 - val_loss: 0.3295 - val_acc: 0.8681\n",
      "Epoch 5/10\n",
      "24896/25000 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8409\n",
      "Epoch 00005: val_loss improved from 0.32948 to 0.31991, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.3768 - acc: 0.8410 - val_loss: 0.3199 - val_acc: 0.8706\n",
      "Epoch 6/10\n",
      "24816/25000 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8417\n",
      "Epoch 00006: val_loss did not improve\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 0.3711 - acc: 0.8417 - val_loss: 0.3255 - val_acc: 0.8723\n",
      "Epoch 7/10\n",
      "24928/25000 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8470\n",
      "Epoch 00007: val_loss did not improve\n",
      "25000/25000 [==============================] - 5s 217us/step - loss: 0.3659 - acc: 0.8470 - val_loss: 0.3346 - val_acc: 0.8748\n",
      "Epoch 8/10\n",
      "24704/25000 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8470\n",
      "Epoch 00008: val_loss did not improve\n",
      "25000/25000 [==============================] - 6s 223us/step - loss: 0.3667 - acc: 0.8471 - val_loss: 0.3252 - val_acc: 0.8725\n",
      "Epoch 9/10\n",
      "24928/25000 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8512\n",
      "Epoch 00009: val_loss improved from 0.31991 to 0.31196, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.3605 - acc: 0.8512 - val_loss: 0.3120 - val_acc: 0.8767\n",
      "Epoch 10/10\n",
      "24896/25000 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8491\n",
      "Epoch 00010: val_loss improved from 0.31196 to 0.30780, saving model to net/best_intermediate_cnn_model.hdf5\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.3580 - acc: 0.8490 - val_loss: 0.3078 - val_acc: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32c8bd09b0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_intermediate_cnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    [train_seq, train_seq_net], train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 10,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = ([test_seq, test_seq_net], test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what I'm talking about. We were able to get a higher accuracy than our base CNN model, **87.75% vs 87.44%**. Stated differently, it looks like our net is better able to understand the subtleties of the English language now, even though we only add just 3 units to the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until know, our results have been lackluster. Even adding in POS information, our Net has yet to eclipse the the **88.704%** results we got from TFiDF, though they came pretty close.\n",
    "\n",
    "This shouldn't come as a surprise.\n",
    "\n",
    "What we're essentially doing here is asking our model to not only analyze the sentiment of the movie reviews, but also derive some understanding of English. Recall that our neural nets have no context. They start not knowing _anything_, like babies, and then have to adapt to the world/problem around them. What would our accuracy look like if we first trained our embeddings against a much larger textual corpus than 50k movie reviews? What if we trained it on... Wikipedia?\n",
    "\n",
    "For this task, I'll make use of 300-dimensional pre-trained [FastText](https://github.com/facebookresearch/fastText). This corpus is trained on the entire Wikipedia circa 2014, which consists of 6Billion tokens, 400K vocabulary words. I hope it performs even better :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "ftext = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('net/wiki.en.vec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426 OOV tokens\n"
     ]
    }
   ],
   "source": [
    "oov = []\n",
    "word_index = tokenizer.word_index\n",
    "nb_words   = min(max_vocab_size, len(word_index))\n",
    "\n",
    "# Any OOV word, will have a 0-vector\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dimension))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_vocab_size or i>=nb_words: break\n",
    "    vec = ftext.get(word)\n",
    "    if vec is None:\n",
    "        oov.append(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = vec\n",
    "        \n",
    "print(len(oov), 'OOV tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it's\",\n",
       " \"don't\",\n",
       " \"i'm\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " '10',\n",
       " \"can't\",\n",
       " \"that's\",\n",
       " \"i've\",\n",
       " \"isn't\",\n",
       " \"there's\",\n",
       " '2',\n",
       " \"he's\",\n",
       " \"wasn't\",\n",
       " '1',\n",
       " \"you're\",\n",
       " '3',\n",
       " \"couldn't\",\n",
       " '5',\n",
       " '4']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting rid of stopwords, which are of little benefit because they are used too often and don't have discriminating power or are used too infrequently and thus won't generalize, we're left with a much smaller vocabulary set with only 4426 OOF tokens. Clearly, we need to do some pre-processing work, such as splitting on \"'\"'s and perhaps mapping numbers from \"10\" -> \"ten\", \"9\" -> \"nine, etc. Maybe next time, since this notebook is getting quite long ...\n",
    "\n",
    "Let's rebuild our first (simple) CNN model, with the only change being the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 250, 300)          13452000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 250, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 248, 32)           28832     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,480,865\n",
      "Trainable params: 28,865\n",
      "Non-trainable params: 13,452,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_cnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "24816/25000 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.7810\n",
      "Epoch 00001: val_loss improved from inf to 0.35198, saving model to net/best_cnn_fasttext_model.hdf5\n",
      "25000/25000 [==============================] - 8s 330us/step - loss: 0.4763 - acc: 0.7812 - val_loss: 0.3520 - val_acc: 0.8559\n",
      "Epoch 2/5\n",
      "24816/25000 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8370\n",
      "Epoch 00002: val_loss improved from 0.35198 to 0.33387, saving model to net/best_cnn_fasttext_model.hdf5\n",
      "25000/25000 [==============================] - 6s 225us/step - loss: 0.3868 - acc: 0.8371 - val_loss: 0.3339 - val_acc: 0.8623\n",
      "Epoch 3/5\n",
      "24960/25000 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8528\n",
      "Epoch 00003: val_loss improved from 0.33387 to 0.32823, saving model to net/best_cnn_fasttext_model.hdf5\n",
      "25000/25000 [==============================] - 6s 232us/step - loss: 0.3598 - acc: 0.8528 - val_loss: 0.3282 - val_acc: 0.8690\n",
      "Epoch 4/5\n",
      "24864/25000 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8585\n",
      "Epoch 00004: val_loss improved from 0.32823 to 0.31414, saving model to net/best_cnn_fasttext_model.hdf5\n",
      "25000/25000 [==============================] - 5s 216us/step - loss: 0.3435 - acc: 0.8584 - val_loss: 0.3141 - val_acc: 0.8747\n",
      "Epoch 5/5\n",
      "24976/25000 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8658\n",
      "Epoch 00005: val_loss improved from 0.31414 to 0.30904, saving model to net/best_cnn_fasttext_model.hdf5\n",
      "25000/25000 [==============================] - 6s 250us/step - loss: 0.3354 - acc: 0.8656 - val_loss: 0.3090 - val_acc: 0.8726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32be511b38>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_cnn_fasttext_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode    = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Net trained with proper embeddings gave better results than the cold-start net, but not better results than the cold-start + NETagger net. You can probably guess the next best idea is to run training on all of Wikipedia, but this time taking into account NETags. Unfortunately, I'm not willing to donate my GPU time for that at this juuncture, but rest assured, the performance will beat TfIDF considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few observations to be made here. First, it's important to have a good understanding of one's problem. The objective here was sentiment analysis. For problems like this, usually, a few words: \"I hated\", \"I loved\", \"very displeased\", \"dissapointed\", \"best movie\", \"unfortunately\", \"won't watch\", \"don't recommend\", \"brilliant film\", etc. are sufficient to identify the sentiment of a comment. Knowing this might help design better model architectures to make the problem space easier to understand.\n",
    "\n",
    "Furthermore, while a chainsaw cuts well, perhaps it's not the _best_ tool to wield while dining :-). Deep learning models perform the best when given a lot of data, but can underpreform vs classical machine learning on smaller datasets, if transfer learning isn't a possibility. At the end of the day, one should start with intuition, but it shoudn't end there. Try everything under the sun.\n",
    "\n",
    "Also, neural nets are non-deterministic if not seeded. Re-running the net with different starting values could result in different accuracy / loss scores. Furthermore, we ran bayesian optimization on tf-idf/lreg but did not do so for our net. Optimizing our layer sizes, learning rate, batch rate, etc. will all have an effect on training.\n",
    "\n",
    "That's all for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
