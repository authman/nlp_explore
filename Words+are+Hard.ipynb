{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words are Hard!\n",
    "### Exploration with DeepNLP for Ontological Word-Sense Disambiguation\n",
    "### By Uthman Apatira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sentence:\n",
    "\n",
    "> _Alexis won the bet agaist her car._\n",
    "\n",
    "Quite confusing even for a native English speaker, much less for a deterministic computer. First of all, who exactly is _her_ referring to here? In English, Alexis is a unisex name, so _her_ might very well be referring to Alexis herself or perhaps some other party. The usage of pronouns makes the sentence appear more natural, but also introduces **referential ambiguity**. The way we (people) normally overcome this is by discourse analysis, that is, by examining the immediately preceeding sentences.\n",
    "\n",
    "Additionally, is the word _bet_ being used as a noun or as a verb? Humans readily identify it as being a noun in this case; but a computer making use of NLP techniques would only have a single feature (TF-IDF) or a single word embedding (Glove/Word2Vec) to describe _bet_ in any context used, noun, verb, or otherwise. This is called **lexical ambiguity**.\n",
    "\n",
    "Finally, the sample sentence above also exhibits **syntactical ambiguity** because it is not clear if Alexis was betting against her own car, i.e. she assumed her car would not be able to fulfill the conditions of the bet, _or_ if Alexis was physically stationed next to her car.\n",
    "\n",
    "Part of the beauty of language is its intrisic ambiguity. It's honestly a meracle we can communite and understand each other at all! This notebook will demonstrate some best practices and dive into a few modern NLP techniques that can help computer models perform better at natural language understanding. Stated formally, this notebook will cover some explaratory data analysis ad modeling with ontological word-sense disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Named Entity Tagging](#Named-Entity-Tagging)\n",
    "- [Classical Sentiment Analysis](#Classical-Sentiment-Analysis)\n",
    "- [Bayesian Hyperparameter Tuning](#Bayesian-Hyperparameter-Tuning)\n",
    "- [Embedding Training](#Embedding-Training)\n",
    "- [Sentiment Analysis with CNNs](#Sentiment-Analysis-with-CNNs)\n",
    "- [Sentiment Analysis with RNNs](#Sentiment-Analysis-with-RNNs)\n",
    "- [Sentiment Analysis with RNNs+CNNs](#Sentiment-Analysis-with-RNNs+CNNs)\n",
    "- [POS-Augmentation for Embeddings](#POS-Augmentation-for-Embeddings)\n",
    "- [Transfer Learning](#Transfer-Learning)\n",
    "- [Reflections](#Reflections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our journey starts with _Named Entity Tagging_, also known as named entity recognition. A named entity is just some entity that deserves to have a name =). The process of tagging them seeks to classify named entities into some pre-defined categories such as the names of persons, organizations, places, etc.\n",
    "\n",
    "By properly tagging named entities, our desire is to have a strong ally in our battle of ontological word-sense disambiguation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, nltk\n",
    "from nltk.chunk import tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downlod some nltk packages, if necessary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_lines = [\n",
    "    'Alexis won the bet agaist her car.',\n",
    "    'Free us from the tyranny of technology, making a connected life a more human experience.',\n",
    "    'He started working at Microsoft in Seattle, Washington.',\n",
    "    'San Francisco has a IBM office.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NETagger(line):\n",
    "    results = []\n",
    "    \n",
    "    # A potnetial improvment :\n",
    "    # Some organizations, e.g. 'JC Penny', places 'San Francisco'\n",
    "    # and name-nouns, 'Brian Kursar' are multi-word. We can \n",
    "    # combine them..\n",
    "\n",
    "    # Tag the individual parts of speech;\n",
    "    line = nltk.pos_tag(line)\n",
    "\n",
    "    # Now, extract named entities via parse tree\n",
    "    line = nltk.ne_chunk(line)\n",
    "\n",
    "    # Finally, covert the tree to a list of\n",
    "    # tag-tuples: (word, pos, tag)\n",
    "    line = nltk.tree2conlltags(line)\n",
    "\n",
    "    for word,pos,tag in line:\n",
    "        if 'GPE' in tag:            tag = 'PLACE'\n",
    "        elif 'ORGANIZATION' in tag: tag = 'ORGANIZATION'\n",
    "        elif 'NN' in pos:           tag = 'NOUN'\n",
    "        elif 'VB' in pos:           tag = 'VERB'\n",
    "        else:                       tag = 'OTHER'\n",
    "        results.append(tag)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for line in sys.stdin:\n",
    "for line in sample_lines:\n",
    "    print('\\x1b[1;31m' + line + '\\x1b[0m')\n",
    "    \n",
    "    # Chop it up, we can use regex to do this directly\n",
    "    line = nltk.word_tokenize(line)\n",
    "    \n",
    "    results = NETagger(line)\n",
    "    for i,word in enumerate(line):\n",
    "        if results[i] == 'OTHER': continue\n",
    "        print(word, 'is', results[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! NLTK's `pos_tag` has support for many more parts of speech, and the `ne_chunk` chunker can identify many more named entities that what we've listed. By for this demo notebook, our dataset will be sufficiently small that if we get too crazy at this point, there potentially won't be enough overlap. That stated, to see the full tagset, execute the cell below. For our purposes, we're limiting the output to just nouns, verbs, places, and organizations. Everything else will just be marked as `'other'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see what it gives us...\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here by 'classical', I mean imploring the use of term-frequency, inverse-term-frequency. We will stack this transformer under either a logistic regression classifier for our final classification.\n",
    "\n",
    "To get us started, I'll be using the `Large Movie Review Dataset`, which is a very balanced dataset of 25k positive and 25k negative reviews of different movies, with no more than 30 reviews per particular movie. So that I don't forget:\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite messy, all the reviews are in separate files. Let's bunch them up into .feather files for easy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, gc, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# For the fancy jupyter status bar\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = os.listdir(\"net/aclImdb/train/neg/\")\n",
    "train_pos = os.listdir(\"net/aclImdb/train/pos/\")\n",
    "test_neg = os.listdir(\"net/aclImdb/test/neg/\")\n",
    "test_pos = os.listdir(\"net/aclImdb/test/pos/\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_pos), len(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "\n",
    "for file in tqdm(train_neg):\n",
    "    with open(\"net/aclImdb/train/neg/\" + file) as fhandler:\n",
    "        train.append([0, fhandler.read()])\n",
    "        \n",
    "for file in tqdm(train_pos):\n",
    "    with open(\"net/aclImdb/train/pos/\" + file) as fhandler:\n",
    "        train.append([1, fhandler.read()])\n",
    "        \n",
    "train = pd.DataFrame(train, columns=['sentiment','comment'])\n",
    "\n",
    "\n",
    "test = []\n",
    "for file in tqdm(test_neg):\n",
    "    with open(\"net/aclImdb/test/neg/\" + file) as fhandler:\n",
    "        test.append([0, fhandler.read()])\n",
    "        \n",
    "for file in tqdm(test_pos):\n",
    "    with open(\"net/aclImdb/test/pos/\" + file) as fhandler:\n",
    "        test.append([1, fhandler.read()])\n",
    "        \n",
    "test  = pd.DataFrame(test, columns=['sentiment','comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stash as feather for easy future access\n",
    "train.to_feather('net/train.ftr')\n",
    "test.to_feather('net/test.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is how to load it in the future\n",
    "train = pd.read_feather('net/train.ftr')\n",
    "test  = pd.read_feather('net/test.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start by training the tdidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tdidf = TfidfVectorizer(\n",
    "    lowercase   = True,\n",
    "    stop_words  = 'english',\n",
    "    ngram_range = (1, 2),\n",
    "    max_df      = 0.4,\n",
    "    min_df      = 5,\n",
    "    binary      = False, \n",
    "    smooth_idf  = True\n",
    ")\n",
    "\n",
    "# We'll train our vctorizer using both train+test\n",
    "tdidf.fit(train.comment.append(test.comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this dataset didn't already provide us a train/test split, we could have\n",
    "# use KFold, and re-ran our model a few times over.\n",
    "train_idf = tdidf.transform(train.comment)\n",
    "test_idf  = tdidf.transform(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, train a LReg classifier over our IDF vector.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "                               \n",
    "lreg = LogisticRegression(\n",
    "    dual    = False,  # For large dsets, this can speed things up . . .\n",
    "    C       = 5.125,  # Our main hyperprameter\n",
    "    n_jobs  = 1\n",
    ")\n",
    "\n",
    "lreg.fit(train_idf, train.sentiment)\n",
    "predict = lreg.predict(test_idf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(test.sentiment, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 88% Accuracy, our score wasn't that great. There are many ways we can improve it. We can add tri-grams to the idf vectorizer. We can also introduce character n-grams as well. No matter what we do, it is always a good idea to tune our hyperparameters using our hold-out set.\n",
    "\n",
    "One technique for doing this is grid-search over our hyperparameter space. This seems to be going out of fashion because Bayesian optimization is all the craze these days, so let's go with that. The deep learner, a 'frequentists', believes a model's parameters are fixed and the data is random. A bayesian learner on the other hand, holds that the _data_ is fixed, but the parameters are random. If you think about it, this makes sense because when a model is trained, the data is fixed and isn't random anymore. I won't go into more detail than that here, but I'll say that there are packages that will take the conditional probabilities out of you :-). Let's use the hyperopt to explore a possible hyperparam space and find the best `C`, `max_df`, and `min_df` values to maximize our accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, define the search space:\n",
    "space = {\n",
    "    'ngram_range': hp.choice('x_ngram_range', [(1,1), (1,2), (1,3)]),\n",
    "    'min_df'     : hp.quniform('x_min_df', 3, 6, 1),\n",
    "    'max_df'     : hp.uniform('x_max_df', 0.4, 1.0),\n",
    "    'C'          : hp.uniform('x_C', 0.1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then define our objective function\n",
    "def objective(space):\n",
    "    # This is where KFold would come into play. We actually have a small\n",
    "    # DSet and using ALL of it (test+train) would be beneficial. We can\n",
    "    # Concatenate both sets, then do KFold x-validation here. But for this\n",
    "    # demo, we'll just use the test set as-is.\n",
    "    global train, test\n",
    "    \n",
    "    # Debugging:\n",
    "    print(space)\n",
    "    \n",
    "    tdidf = TfidfVectorizer(\n",
    "        lowercase   = True,\n",
    "        stop_words  = 'english',\n",
    "        ngram_range = space['ngram_range'],\n",
    "        max_df      = space['max_df'],\n",
    "        min_df      = int(space['min_df']),\n",
    "        binary      = False, \n",
    "        smooth_idf  = True\n",
    "    )\n",
    "    tdidf.fit(train.comment.append(test.comment))\n",
    "    \n",
    "    lreg = LogisticRegression(\n",
    "        dual    = False,\n",
    "        C       = space['C']\n",
    "    )\n",
    "    lreg.fit(train_idf, train.sentiment)\n",
    "    \n",
    "    predict = lreg.predict(test_idf)\n",
    "    score   = accuracy_score(test.sentiment, predict)\n",
    "    print('\\t', score, '\\n')\n",
    "    \n",
    "    # 1-score, since we're going to attempt to minimize this value\n",
    "    return 1.0 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Trials object will store details of each iteration\n",
    "trials = Trials()\n",
    "\n",
    "# Run it:\n",
    "best = fmin(\n",
    "    objective,\n",
    "    space     = space,\n",
    "    algo      = tpe.suggest,\n",
    "    max_evals = 10\n",
    ")\n",
    "\n",
    "# Get the values of the optimal parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, it looks like the tuned-TfIDF gets us all the way up to 88.704% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten as far as we believe we can with TFIDF, let's move on to neural networks.\n",
    "\n",
    "We could feed in our sparse TF-IDF representation as input into our net (think one hot encoding). But instead, we're going to try to create an embedding, or lower level representation of our language data. This will involve us first tokenizing our textual corpus into indices, and then assigning a random, trainable weight vector to each word.\n",
    "\n",
    "Before starting, we're going to need to set a fixed input size for our comments. How do we know what size to use? Let's see what the data tells us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = train.comment.str.count('\\S+').astype(np.uint16)\n",
    "\n",
    "plt.hist(words, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**250** will be a very competitive number of words. It'll capture most of the next of most of our comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding, Conv1D \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size  = 100000   # We won't use more than 100K words\n",
    "max_text_len    = 250      # We'll only examine up to the first 100 words of each review\n",
    "\n",
    "embedding_dimension = 100    # Each word will be represented with a dense, 100 unit vector\n",
    "embedding_maxvocab  = 500000 # Our language model will know up to 500k words, probably more than in our toy dset\n",
    "embedding_cbow_win  = 5      # Window size for unsuperivsd text context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = train.comment.append(test.comment).values\n",
    "all_comments = [text.text_to_word_sequence(comment) for comment in tqdm(all_comments)]\n",
    "\n",
    "w2v = Word2Vec(size=embedding_dimension, window=embedding_cbow_win, max_vocab_size=embedding_maxvocab)\n",
    "w2v.build_vocab(all_comments)\n",
    "w2v.train(all_comments, total_examples=w2v.corpus_count, epochs=5)\n",
    "\n",
    "w2v.save('net/embedding.w2v')\n",
    "\n",
    "del all_comments; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we allow our embedding vocabulary to be quite big, but this is because while building the corpus, we don't know which words are important yet. Later on once we start supervised learning, we can prune accordingly.\n",
    "\n",
    "Next up, let's build a mapper to tokenize our text (change the words -> vocab indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(\n",
    "    list(train.comment.fillna('NA').values) + list(test.comment.fillna('NA').values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenizer):\n",
    "    # Just like we did with TfIdf, let's remove stopwords\n",
    "    # From our vocabulary\n",
    "    \n",
    "    C_MIN_WF = 4\n",
    "    C_MIN_DF = 4\n",
    "    C_MAX_DF = tokenizer.document_count * 0.4\n",
    "    print('MAX_DF', C_MAX_DF)\n",
    "\n",
    "    whack_words = [w for w,c in tokenizer.word_counts.items() if c < C_MIN_WF] + [w for w,c in tokenizer.word_docs.items() if c < C_MIN_DF or c > C_MAX_DF]\n",
    "    whack_words = list(set(whack_words))\n",
    "    for word in whack_words:\n",
    "        del tokenizer.word_counts[word], tokenizer.word_index[word], tokenizer.word_docs[word]\n",
    "        #tokenizer.num_words -= 1\n",
    "\n",
    "    # Since we deleted some entris, relabel (fix) the word indices\n",
    "    word_index_keys = tokenizer.word_index.keys()\n",
    "    for i,word in enumerate(word_index_keys):\n",
    "        tokenizer.word_index[word] = i+1\n",
    "    print('Deleting', len(whack_words), 'words that appear too frequently or too infrequently')\n",
    "\n",
    "    word_index_keys = tokenizer.word_index.keys()\n",
    "    print(len(word_index_keys), 'words left')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = remove_stopwords(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We just built the model, so no need to load it.\n",
    "# But this is how that would be done:\n",
    "w2v = Word2Vec.load('net/embedding.w2v')\n",
    "\n",
    "\n",
    "# Start by initializing our embedding matrix [VocabSize x EmbeddingDimension]\n",
    "word_index = tokenizer.word_index\n",
    "nb_words   = min(max_vocab_size, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dimension), dtype=np.float64)\n",
    "\n",
    "# Since we're using a self trained embedding, our expectation is that there\n",
    "# will be no out-of-vocabulary words present... but I'll leave that code in\n",
    "# there anyway because I'll be coming back to it:\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_vocab_size: continue\n",
    "    try:\n",
    "        embedding_vector = w2v.wv[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can execute the tokenizer:\n",
    "train_seq = tokenizer.texts_to_sequences(train.comment)\n",
    "test_seq  = tokenizer.texts_to_sequences(test.comment)\n",
    "\n",
    "# Add 0-padding to reach max_text_len size\n",
    "train_seq = sequence.pad_sequences(train_seq, maxlen=max_text_len)\n",
    "test_seq  = sequence.pad_sequences(test_seq,  maxlen=max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word \"I\" is mapped to this 100-unit vector:\n",
    "train.comment[0][0], train_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of prep work, but now we're ready to build a NNet model to perform sentiment analysis. For the loss function, I'll use binary log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def simple_cnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_cnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple model. Notice that our embedding layer is now frozen. Our understanding is that the embedding layer encoders some information about 'English', which includes syntax, semantics, and pragmatics to some degree. We don't want to change that. Rather, we'll leave it to our convolutional filters to learn the target mapping of good / bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_cnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 10,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our convlutional model performed worse than our classical IDF-LReg model, how disappointing. Some things to note -- we used `kernel_size=3`, and don't have any spatial pooling (only temporal pooling). You can imagine here we're looking at n=3 grams. Our TFIDF model used n=1,2, and 3 gram combinations. We could adjust our nnet with additional kernels to consider additional n-grams as well, but that won't be explored in this notebook. There are many other ways to improve our network as well, but let's take a look at an RNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_rnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(32//2, return_sequences=True))(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_rnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_rnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since at train and test time, we have the entire sentence available, it makes sense that we use a bidirectional rnn rather than just limiting ourselves to processing the sentence unidirectionally.\n",
    "\n",
    "But take a look at that training time. RNNs are a lot slower than CNNs because they consume data recursively. There is some research out there on [helping RNNs perform better in parallel](NLP QRNN: https://arxiv.org/pdf/1803.08240.pdf), but it's is not widely adopted as of yet. We can use a larger batch size to speed things up, but that affects training quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis II with RNNs+CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's see what happens when we combine both RNN and CNN processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_rcnn_model(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape = (max_text_len, ))\n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(32//2, return_sequences=True))(x)\n",
    "    x = Conv1D(filters=64, kernel_size=1, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_rnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_rcnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Augmentation for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can further improve our NNet scores by adding in either named entities or the part of speech information we worked on earlier? There are a few techniques we could use. One would be to train an embedding on our original text corpus converted into POS or NETs. Then, armed with an embedding to describe each distinct POS, we could simply cncatenate that onto our input. Another methods would be to OHE the tags and just feed them in as a separate input vector. Let's try out the first method.\n",
    "\n",
    "But first, a quick \"Gotcha!\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.word_tokenize(\"He started working at Microsoft in Seattle, Washington.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text.text_to_word_sequence(\"He started working at Microsoft in Seattle, Washington.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Keras's text tokenizer converts everything to lowercase? That will mess up our NET. So let's use nltk's tokenizer this round instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netag_max_vocab_size       = 6   # place, org, noun, verb, other (0 is empty)\n",
    "netag_embedding_dimension  = 3   # Meh, should do it. min(50, dimen//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process takes a while, so let's use all our (my) available CPU threads to speed it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def build_ne_tags():\n",
    "    global train, test\n",
    "    \n",
    "    NUM_THREADS = 12\n",
    "\n",
    "    all_ne_tags = train.comment.append(test.comment).values\n",
    "    with Pool(NUM_THREADS) as pool:  all_ne_tags = pool.map(nltk.word_tokenize, all_ne_tags)\n",
    "    with Pool(NUM_THREADS) as pool:  all_ne_tags = pool.map(NETagger, all_ne_tags)\n",
    "        \n",
    "    return all_ne_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're in business :-)\n",
    "all_ne_tags = build_ne_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_tag = Word2Vec(size=netag_embedding_dimension, window=embedding_cbow_win, max_vocab_size=netag_max_vocab_size)\n",
    "w2v_tag.build_vocab(all_ne_tags)\n",
    "w2v_tag.train(all_ne_tags, total_examples=w2v.corpus_count, epochs=5)\n",
    "\n",
    "# Store resulting embedding\n",
    "w2v_tag.save('net/net_embedding.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case we're coming in from another run where we have it saved already:\n",
    "w2v_tag = Word2Vec.load('net/net_embedding.w2v')\n",
    "\n",
    "# Stash the data back into our dset\n",
    "train['netags'] = all_ne_tags[:train.shape[0]]\n",
    "test['netags']  = all_ne_tags[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, rebuild the CNN model to take dual inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_tag.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = {\n",
    "    'oov':          0,\n",
    "    'NOUN':         1,\n",
    "    'ORGANIZATION': 2,\n",
    "    'OTHER':        3,\n",
    "    'PLACE':        4,\n",
    "    'VERB':         5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by initializing our embedding matrix [VocabSize x EmbeddingDimension]\n",
    "net_embedding_matrix = np.zeros((netag_max_vocab_size, netag_embedding_dimension))\n",
    "\n",
    "# Self trained, no OOV:\n",
    "for key,val in tagger.items():\n",
    "    try:\n",
    "        net_embedding_matrix[val] = w2v_tag.wv[key]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cool !\n",
    "net_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq_net = train.netags.map(lambda tags: [tagger[tag] for tag in tags])\n",
    "test_seq_net  = test.netags.map(lambda tags: [tagger[tag] for tag in tags])\n",
    "\n",
    "# Add 0-padding to reach max_text_len size\n",
    "train_seq_net = sequence.pad_sequences(train_seq_net, maxlen=max_text_len)\n",
    "test_seq_net  = sequence.pad_sequences(test_seq_net,  maxlen=max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup our dual-head CNN:\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def intermediate_cnn_model(embedding_matrix, net_embedding_matrix):\n",
    "    \n",
    "    inp  = Input(shape = (max_text_len, ))\n",
    "    inp2 = Input(shape = (max_text_len, ))\n",
    "    \n",
    "    x = Embedding(\n",
    "        nb_words,\n",
    "        embedding_dimension,\n",
    "        weights      = [embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp)\n",
    "    \n",
    "    y = Embedding(\n",
    "        netag_max_vocab_size,\n",
    "        netag_embedding_dimension,\n",
    "        weights      = [net_embedding_matrix],\n",
    "        input_length = max_text_len,\n",
    "        trainable    = False\n",
    "    )(inp2)\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = concatenate([\n",
    "        # Text + NETags\n",
    "        x, y\n",
    "    ])\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x) # Squash\n",
    "    \n",
    "    model = Model(inputs=[inp,inp2], outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Nadam(lr=0.01, clipvalue=0.5),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = intermediate_cnn_model(embedding_matrix, net_embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_intermediate_cnn_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    [train_seq, train_seq_net], train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 10,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = ([test_seq, test_seq_net], test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wadaya know, it performs considerably better =) !\n",
    "\n",
    "Stated differently, our net is better able to understand the subtleties of the English language now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until know, our results have been lackluster. Even adding in POS information, our Net has yet to eclipse the results we got from TFiDF, though they came pretty close.\n",
    "\n",
    "This shouldn't come as a surprise.\n",
    "\n",
    "What we're essentially doing here is asking our model to not only analyze the sentiment of the movie reviews, but also derive some understanding of English. Recall that our neural nets have no context. They start not knowing _anything_, like babies, and then have to adapt to the world/problem around them. What would our accuracy look like if we first trained our embeddings against a much larger textual corpus than 50k movie reviews? What if we trained it on... Wikipedia?\n",
    "\n",
    "For this task, I'll make use of 300-dimensional pre-trained [FastText](https://github.com/facebookresearch/fastText). This corpus is trained on the entire Wikipedia circa 2014, which consists of 6Billion tokens, 400K vocabulary words. I hope it performs even better :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "glove = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('net/wiki.en.vec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "word_index = tokenizer.word_index\n",
    "nb_words   = min(max_vocab_size, len(word_index))\n",
    "\n",
    "# Any OOV word, will have a 0-vector\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dimension))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_vocab_size or i>=nb_words: break\n",
    "    vec = glove.get(word)\n",
    "    if vec is None:\n",
    "        oov.append(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = vec\n",
    "        \n",
    "print(len(oov), 'OOV tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting rid of stopwords, which are of little benefit because they are used too often and don't have discriminating power or are used too infrequently and thus won't generalize, we're left with a much smaller vocabulary set with only 4426 OOF tokens. Clearly, we need to do some pre-processing work, such as splitting on \"'\"'s and perhaps mapping numbers from \"10\" -> \"ten\", \"9\" -> \"nine, etc. Maybe next time, since this notebook is getting quite long ...\n",
    "\n",
    "Let's rebuild our CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = simple_cnn_model(embedding_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_point = ModelCheckpoint(\n",
    "    'net/best_cnn_fasttext_model.hdf5',\n",
    "    monitor = \"val_loss\",\n",
    "    mode    = \"min\",\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_seq, train.sentiment,\n",
    "    batch_size = 16,\n",
    "    epochs     = 5,\n",
    "    verbose    = 1,\n",
    "    callbacks  = [check_point],\n",
    "    validation_data = (test_seq, test.sentiment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Net trained with proper embeddings gave better results than the cold-start net, but not better results than the cold-start + NETagger net. You can probably guess the next best idea is to run training on all of Wikipedia, but this time taking into account NETags. Unfortunately, I'm not willing to donate my GPU time for that at this juuncture, but rest assured, the performance will beat TfIDF considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few observations to be made here. First, it's important to have a good understanding of one's problem. The objective here was sentiment analysis. For problems like this, usually, a few words: \"I hated\", \"I loved\", \"very displeased\", \"dissapointed\", \"best movie\", \"unfortunately\", \"won't watch\", \"don't recommend\", \"brilliant film\", etc. are sufficient to identify the sentiment of a comment. Knowing this might help design better model architectures to make the problem space easier to understand.\n",
    "\n",
    "Furthermore, while a chainsaw cuts well, perhaps it's not the _best_ tool to wield while dining :-). Deep learning models perform the best when given a lot of data, but can underpreform vs classical machine learning on smaller datasets, if transfer learning isn't a possibility. At the end of the day, one should start with intuition, but it shoudn't end there. Try everything under the sun.\n",
    "\n",
    "Also, neural nets are non-deterministic if not seeded. Re-running the net with different starting values could result in different accuracy / loss scores. Furthermore, we ran bayesian optimization on tf-idf/lreg but did not do so for our net. Optimizing our layer sizes, learning rate, batch rate, etc. will all have an effect on training.\n",
    "\n",
    "That's all for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
